{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:43:55.386546Z",
     "iopub.status.busy": "2022-05-23T00:43:55.385606Z",
     "iopub.status.idle": "2022-05-23T00:44:03.042600Z",
     "shell.execute_reply": "2022-05-23T00:44:03.041733Z",
     "shell.execute_reply.started": "2022-05-23T00:43:55.386404Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "import string\n",
    "import unicodedata\n",
    "from random import randint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#nlk libraries\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import STOPWORDS, WordCloud\n",
    "\n",
    "# sklearn Libraries \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# tensorflow Libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Input, Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Embedding, TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:03.044473Z",
     "iopub.status.busy": "2022-05-23T00:44:03.044124Z",
     "iopub.status.idle": "2022-05-23T00:44:13.219728Z",
     "shell.execute_reply": "2022-05-23T00:44:13.218857Z",
     "shell.execute_reply.started": "2022-05-23T00:44:03.044439Z"
    }
   },
   "outputs": [],
   "source": [
    "!pip install -q contractions==0.0.48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:13.222568Z",
     "iopub.status.busy": "2022-05-23T00:44:13.222123Z",
     "iopub.status.idle": "2022-05-23T00:44:13.239644Z",
     "shell.execute_reply": "2022-05-23T00:44:13.238705Z",
     "shell.execute_reply.started": "2022-05-23T00:44:13.222521Z"
    }
   },
   "outputs": [],
   "source": [
    "CONTRACTION_MAP = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \n",
    "                   \"can't've\": \"cannot have\", \"'cause\": \"because\", \"could've\": \"could have\", \n",
    "                   \"couldn't\": \"could not\", \"couldn't've\": \"could not have\",\"didn't\": \"did not\", \n",
    "                   \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \n",
    "                   \"hadn't've\": \"had not have\", \"hasn't\": \"has not\", \"haven't\": \"have not\", \n",
    "                   \"he'd\": \"he would\", \"he'd've\": \"he would have\", \"he'll\": \"he will\", \n",
    "                   \"he'll've\": \"he he will have\", \"he's\": \"he is\", \"how'd\": \"how did\", \n",
    "                   \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\", \n",
    "                   \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \n",
    "                   \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \n",
    "                   \"i'd\": \"i would\", \"i'd've\": \"i would have\", \"i'll\": \"i will\", \n",
    "                   \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \n",
    "                   \"isn't\": \"is not\", \"it'd\": \"it would\", \"it'd've\": \"it would have\", \n",
    "                   \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \n",
    "                   \"let's\": \"let us\", \"ma'am\": \"madam\", \"mayn't\": \"may not\", \n",
    "                   \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \n",
    "                   \"must've\": \"must have\", \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \n",
    "                   \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\", \n",
    "                   \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\",\n",
    "                   \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\", \"she'd\": \"she would\", \n",
    "                   \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \n",
    "                   \"she's\": \"she is\", \"should've\": \"should have\", \"shouldn't\": \"should not\", \n",
    "                   \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\", \n",
    "                   \"this's\": \"this is\",\n",
    "                   \"that'd\": \"that would\", \"that'd've\": \"that would have\",\"that's\": \"that is\", \n",
    "                   \"there'd\": \"there would\", \"there'd've\": \"there would have\",\"there's\": \"there is\", \n",
    "                   \"they'd\": \"they would\", \"they'd've\": \"they would have\", \"they'll\": \"they will\", \n",
    "                   \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \n",
    "                   \"to've\": \"to have\", \"wasn't\": \"was not\", \"we'd\": \"we would\", \n",
    "                   \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \n",
    "                   \"we're\": \"we are\", \"we've\": \"we have\", \"weren't\": \"were not\", \n",
    "                   \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\", \n",
    "                   \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \n",
    "                   \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\", \n",
    "                   \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \n",
    "                   \"who's\": \"who is\", \"who've\": \"who have\", \"why's\": \"why is\", \n",
    "                   \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \n",
    "                   \"won't've\": \"will not have\", \"would've\": \"would have\", \"wouldn't\": \"would not\", \n",
    "                   \"wouldn't've\": \"would not have\", \"y'all\": \"you all\", \"y'all'd\": \"you all would\",\n",
    "                   \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n",
    "                   \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \n",
    "                   \"you'll've\": \"you will have\", \"you're\": \"you are\", \"you've\": \"you have\" } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:13.242811Z",
     "iopub.status.busy": "2022-05-23T00:44:13.242478Z",
     "iopub.status.idle": "2022-05-23T00:44:19.463208Z",
     "shell.execute_reply": "2022-05-23T00:44:19.462302Z",
     "shell.execute_reply.started": "2022-05-23T00:44:13.242769Z"
    }
   },
   "outputs": [],
   "source": [
    "# Using TPU\n",
    "\n",
    "# detect and init the TPU\n",
    "tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
    "tf.config.experimental_connect_to_cluster(tpu)\n",
    "tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "\n",
    "# instantiate a distribution strategy\n",
    "tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:19.465018Z",
     "iopub.status.busy": "2022-05-23T00:44:19.464717Z",
     "iopub.status.idle": "2022-05-23T00:44:19.469941Z",
     "shell.execute_reply": "2022-05-23T00:44:19.468895Z",
     "shell.execute_reply.started": "2022-05-23T00:44:19.464978Z"
    }
   },
   "outputs": [],
   "source": [
    "fn1 = '../input/news-summary/news_summary.csv'\n",
    "fn2 = '../input/news-summary/news_summary_more.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:19.472141Z",
     "iopub.status.busy": "2022-05-23T00:44:19.471832Z",
     "iopub.status.idle": "2022-05-23T00:44:20.932736Z",
     "shell.execute_reply": "2022-05-23T00:44:20.931733Z",
     "shell.execute_reply.started": "2022-05-23T00:44:19.472100Z"
    }
   },
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(fn1, encoding='iso-8859-1').reset_index(drop=True)\n",
    "df2 = pd.read_csv(fn2, encoding='iso-8859-1').reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:20.934376Z",
     "iopub.status.busy": "2022-05-23T00:44:20.934085Z",
     "iopub.status.idle": "2022-05-23T00:44:20.990440Z",
     "shell.execute_reply": "2022-05-23T00:44:20.989564Z",
     "shell.execute_reply.started": "2022-05-23T00:44:20.934345Z"
    }
   },
   "outputs": [],
   "source": [
    "df1_columns = df1.columns.tolist()\n",
    "df1_columns.remove('headlines')\n",
    "df1_columns.remove('text')\n",
    "df1.drop(df1_columns, axis='columns', inplace=True)\n",
    "df = pd.concat([df1, df2], axis='rows')\n",
    "del df1, df2\n",
    "df = df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:20.991852Z",
     "iopub.status.busy": "2022-05-23T00:44:20.991621Z",
     "iopub.status.idle": "2022-05-23T00:44:20.998863Z",
     "shell.execute_reply": "2022-05-23T00:44:20.997927Z",
     "shell.execute_reply.started": "2022-05-23T00:44:20.991825Z"
    }
   },
   "outputs": [],
   "source": [
    "def lemmatise_words(text, cmap=CONTRACTION_MAP):\n",
    " \n",
    "    ckeys = '|'.join(cmap.keys())\n",
    "    patterns = re.compile(f'({ckeys})', flags=re.DOTALL)\n",
    "\n",
    "    def lemmatise(contraction):\n",
    "\n",
    "        match = contraction.group(0)\n",
    "        lemattised = cmap.get(match)\n",
    "        if not lemmatise_words:\n",
    "            return match\n",
    "        return lemattised\n",
    "\n",
    "    lemmatise_text = patterns.sub(lemmatise, text)\n",
    "    lemmatise_text = re.sub(\"'\", \"\", lemmatise_text)\n",
    "    return lemmatise_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:21.000288Z",
     "iopub.status.busy": "2022-05-23T00:44:21.000033Z",
     "iopub.status.idle": "2022-05-23T00:44:21.219385Z",
     "shell.execute_reply": "2022-05-23T00:44:21.218490Z",
     "shell.execute_reply.started": "2022-05-23T00:44:21.000260Z"
    }
   },
   "outputs": [],
   "source": [
    "# Lowercase\n",
    "df.text = df.text.apply(str.lower)\n",
    "df.headlines = df.headlines.apply(str.lower)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:21.222379Z",
     "iopub.status.busy": "2022-05-23T00:44:21.222019Z",
     "iopub.status.idle": "2022-05-23T00:44:37.681761Z",
     "shell.execute_reply": "2022-05-23T00:44:37.680903Z",
     "shell.execute_reply.started": "2022-05-23T00:44:21.222343Z"
    }
   },
   "outputs": [],
   "source": [
    "#Lemmatisation\n",
    "df.headlines = df.headlines.apply(lemmatise_words)\n",
    "df.text = df.text.apply(lemmatise_words)\n",
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:37.683722Z",
     "iopub.status.busy": "2022-05-23T00:44:37.683246Z",
     "iopub.status.idle": "2022-05-23T00:44:37.688942Z",
     "shell.execute_reply": "2022-05-23T00:44:37.688158Z",
     "shell.execute_reply.started": "2022-05-23T00:44:37.683689Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove puncuation from word\n",
    "def punctuation_removal(word):\n",
    "    clean_lst = []\n",
    "    for alphabet in word:\n",
    "        if alphabet not in string.punctuation:\n",
    "            clean_lst.append(alphabet)\n",
    "        \n",
    "    return ''.join(clean_lst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:37.690869Z",
     "iopub.status.busy": "2022-05-23T00:44:37.690429Z",
     "iopub.status.idle": "2022-05-23T00:44:37.717786Z",
     "shell.execute_reply": "2022-05-23T00:44:37.717002Z",
     "shell.execute_reply.started": "2022-05-23T00:44:37.690839Z"
    }
   },
   "outputs": [],
   "source": [
    "def rm_punc_from_text(text):\n",
    "    clean_word_list = [punctuation_removal(word) for word in text]\n",
    "    return ''.join(clean_word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:37.719742Z",
     "iopub.status.busy": "2022-05-23T00:44:37.719280Z",
     "iopub.status.idle": "2022-05-23T00:44:37.741165Z",
     "shell.execute_reply": "2022-05-23T00:44:37.740342Z",
     "shell.execute_reply.started": "2022-05-23T00:44:37.719711Z"
    }
   },
   "outputs": [],
   "source": [
    "def number_removal(text):\n",
    "    text = re.sub('[0-9]+', '', text)\n",
    "    return ' '.join(text.split()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:37.743651Z",
     "iopub.status.busy": "2022-05-23T00:44:37.743187Z",
     "iopub.status.idle": "2022-05-23T00:44:37.763740Z",
     "shell.execute_reply": "2022-05-23T00:44:37.762954Z",
     "shell.execute_reply.started": "2022-05-23T00:44:37.743607Z"
    }
   },
   "outputs": [],
   "source": [
    "# Remove stopwords from text\n",
    "def stopword_removal(text):\n",
    "    _stopwords = stopwords.words('english')\n",
    "    text = text.split()\n",
    "    word_list = [word for word in text if word not in _stopwords]\n",
    "    return ' '.join(word_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:37.765492Z",
     "iopub.status.busy": "2022-05-23T00:44:37.765095Z",
     "iopub.status.idle": "2022-05-23T00:44:37.782741Z",
     "shell.execute_reply": "2022-05-23T00:44:37.781686Z",
     "shell.execute_reply.started": "2022-05-23T00:44:37.765450Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning text\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    text = rm_punc_from_text(text)\n",
    "    text = number_removal(text)\n",
    "    text = stopword_removal(text)\n",
    "\n",
    "    text = re.sub('–', '', text)\n",
    "    text = ' '.join(text.split()) \n",
    "    text = re.sub(\"(\\\\t)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\\\r)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\\\n)\", ' ', str(text)).lower()\n",
    "\n",
    "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode(\n",
    "        'utf-8', 'ignore'\n",
    "    )\n",
    "\n",
    "    text = re.sub(\"(__+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(--+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(~~+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\+\\++)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\.\\.+)\", ' ', str(text)).lower()\n",
    "\n",
    "    text = re.sub(r\"[<>()|&©ø\\[\\]\\'\\\",;?~*!]\", ' ', str(text)).lower()\n",
    "\n",
    "    text = re.sub(\"(mailto:)\", ' ', str(text)).lower()\n",
    "    text = re.sub(r\"(\\\\x9\\d)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', str(text)).lower()\n",
    "    text = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM',\n",
    "                  str(text)).lower()\n",
    "\n",
    "    text = re.sub(\"(\\.\\s+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\-\\s+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\:\\s+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n",
    "\n",
    "    try:\n",
    "        url = re.search(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', str(text))\n",
    "        repl_url = url.group(3)\n",
    "        text = re.sub(r'((https*:\\/*)([^\\/\\s]+))(.[^\\s]+)', repl_url, str(text))\n",
    "    except Exception as e:\n",
    "        pass\n",
    "\n",
    "    text = re.sub(\"(\\s+)\", ' ', str(text)).lower()\n",
    "    text = re.sub(\"(\\s+.\\s+)\", ' ', str(text)).lower()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:44:37.784682Z",
     "iopub.status.busy": "2022-05-23T00:44:37.784107Z",
     "iopub.status.idle": "2022-05-23T00:46:12.221982Z",
     "shell.execute_reply": "2022-05-23T00:46:12.220951Z",
     "shell.execute_reply.started": "2022-05-23T00:44:37.784636Z"
    }
   },
   "outputs": [],
   "source": [
    "df.text = df.text.apply(clean_text)\n",
    "df.headlines = df.headlines.apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:12.224183Z",
     "iopub.status.busy": "2022-05-23T00:46:12.223420Z",
     "iopub.status.idle": "2022-05-23T00:46:12.278418Z",
     "shell.execute_reply": "2022-05-23T00:46:12.277432Z",
     "shell.execute_reply.started": "2022-05-23T00:46:12.224146Z"
    }
   },
   "outputs": [],
   "source": [
    "df.headlines = df.headlines.apply(lambda x: f'_START_ {x} _END_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:12.280484Z",
     "iopub.status.busy": "2022-05-23T00:46:12.280144Z",
     "iopub.status.idle": "2022-05-23T00:46:12.342115Z",
     "shell.execute_reply": "2022-05-23T00:46:12.341480Z",
     "shell.execute_reply.started": "2022-05-23T00:46:12.280442Z"
    }
   },
   "outputs": [],
   "source": [
    "start_token = 'sostok'\n",
    "end_token = 'eostok'\n",
    "df.headlines = df.headlines.apply(lambda x: f'{start_token} {x} {end_token}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:12.343820Z",
     "iopub.status.busy": "2022-05-23T00:46:12.343062Z",
     "iopub.status.idle": "2022-05-23T00:46:12.355472Z",
     "shell.execute_reply": "2022-05-23T00:46:12.354589Z",
     "shell.execute_reply.started": "2022-05-23T00:46:12.343782Z"
    }
   },
   "outputs": [],
   "source": [
    "df.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:12.357063Z",
     "iopub.status.busy": "2022-05-23T00:46:12.356812Z",
     "iopub.status.idle": "2022-05-23T00:46:13.539792Z",
     "shell.execute_reply": "2022-05-23T00:46:13.538984Z",
     "shell.execute_reply.started": "2022-05-23T00:46:12.357035Z"
    }
   },
   "outputs": [],
   "source": [
    "text_count = [len(sentence.split()) for sentence in df.text]\n",
    "headlines_count = [len(sentence.split()) for sentence in df.headlines]\n",
    "\n",
    "pd.DataFrame({'text': text_count, 'headlines': headlines_count}).hist(bins=100, figsize=(16, 4), range=[0, 50])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:13.541090Z",
     "iopub.status.busy": "2022-05-23T00:46:13.540890Z",
     "iopub.status.idle": "2022-05-23T00:46:13.544899Z",
     "shell.execute_reply": "2022-05-23T00:46:13.544291Z",
     "shell.execute_reply.started": "2022-05-23T00:46:13.541065Z"
    }
   },
   "outputs": [],
   "source": [
    "max_text_len = 42\n",
    "max_summary_len = 13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:13.546418Z",
     "iopub.status.busy": "2022-05-23T00:46:13.546157Z",
     "iopub.status.idle": "2022-05-23T00:46:13.994412Z",
     "shell.execute_reply": "2022-05-23T00:46:13.993723Z",
     "shell.execute_reply.started": "2022-05-23T00:46:13.546372Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def trim_text_and_summary(df, max_text_len, max_summary_len):\n",
    "    cleaned_text = np.array(df['text'])\n",
    "    cleaned_summary = np.array(df['headlines'])\n",
    "\n",
    "    short_text = []\n",
    "    short_summary = []\n",
    "\n",
    "    for i in range(len(cleaned_text)):\n",
    "        if len(cleaned_text[i].split()) <= max_text_len and len(\n",
    "            cleaned_summary[i].split()\n",
    "        ) <= max_summary_len:\n",
    "            short_text.append(cleaned_text[i])\n",
    "            short_summary.append(cleaned_summary[i])\n",
    "\n",
    "    df = pd.DataFrame({'text': short_text, 'summary': short_summary})\n",
    "    return df\n",
    "\n",
    "\n",
    "df = trim_text_and_summary(df, max_text_len, max_summary_len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:13.996681Z",
     "iopub.status.busy": "2022-05-23T00:46:13.995981Z",
     "iopub.status.idle": "2022-05-23T00:46:14.002874Z",
     "shell.execute_reply": "2022-05-23T00:46:14.002283Z",
     "shell.execute_reply.started": "2022-05-23T00:46:13.996632Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_word_percent_which_are_very_rare(tokeniser, threshold):\n",
    "    \n",
    "    \n",
    "    count = 0\n",
    "    frequency = 0\n",
    "    \n",
    "    \n",
    "    tf = 0\n",
    "    total_count = 0\n",
    "\n",
    "    for word, count in tokeniser.word_counts.items():\n",
    "        total_count += 1\n",
    "        tf += count\n",
    "        if count < threshold:\n",
    "            count += 1\n",
    "            frequency += count\n",
    "\n",
    "    return {\n",
    "        'percent': round((count / total_count) * 100, 2)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:14.004521Z",
     "iopub.status.busy": "2022-05-23T00:46:14.003878Z",
     "iopub.status.idle": "2022-05-23T00:46:14.039720Z",
     "shell.execute_reply": "2022-05-23T00:46:14.038581Z",
     "shell.execute_reply.started": "2022-05-23T00:46:14.004472Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train, validation_x, y_train, validation_y = train_test_split(np.array(df['text']),np.array(df['summary']),test_size=0.1,random_state=1,shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:14.041288Z",
     "iopub.status.busy": "2022-05-23T00:46:14.040929Z",
     "iopub.status.idle": "2022-05-23T00:46:18.152017Z",
     "shell.execute_reply": "2022-05-23T00:46:18.151092Z",
     "shell.execute_reply.started": "2022-05-23T00:46:14.041251Z"
    }
   },
   "outputs": [],
   "source": [
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_train))\n",
    "\n",
    "x_tokens_data = get_word_percent_which_are_very_rare(x_tokenizer, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:18.154363Z",
     "iopub.status.busy": "2022-05-23T00:46:18.153345Z",
     "iopub.status.idle": "2022-05-23T00:46:22.315444Z",
     "shell.execute_reply": "2022-05-23T00:46:22.314537Z",
     "shell.execute_reply.started": "2022-05-23T00:46:18.154322Z"
    }
   },
   "outputs": [],
   "source": [
    "x_tokenizer = Tokenizer()\n",
    "x_tokenizer.fit_on_texts(list(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:22.317541Z",
     "iopub.status.busy": "2022-05-23T00:46:22.316809Z",
     "iopub.status.idle": "2022-05-23T00:46:25.917039Z",
     "shell.execute_reply": "2022-05-23T00:46:25.915963Z",
     "shell.execute_reply.started": "2022-05-23T00:46:22.317490Z"
    }
   },
   "outputs": [],
   "source": [
    "# One-hot-encoding\n",
    "x_train_sequence = x_tokenizer.texts_to_sequences(x_train)\n",
    "validation_x_sequence = x_tokenizer.texts_to_sequences(validation_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:25.922818Z",
     "iopub.status.busy": "2022-05-23T00:46:25.922555Z",
     "iopub.status.idle": "2022-05-23T00:46:27.295248Z",
     "shell.execute_reply": "2022-05-23T00:46:27.294307Z",
     "shell.execute_reply.started": "2022-05-23T00:46:25.922789Z"
    }
   },
   "outputs": [],
   "source": [
    "# Padding\n",
    "x_train_padded = pad_sequences(x_train_sequence, maxlen=max_text_len, padding='post')\n",
    "validation_x_padded = pad_sequences(validation_x_sequence, maxlen=max_text_len, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:27.296752Z",
     "iopub.status.busy": "2022-05-23T00:46:27.296513Z",
     "iopub.status.idle": "2022-05-23T00:46:27.301307Z",
     "shell.execute_reply": "2022-05-23T00:46:27.300483Z",
     "shell.execute_reply.started": "2022-05-23T00:46:27.296725Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "x_size = len(x_tokenizer.word_index) + 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:27.302804Z",
     "iopub.status.busy": "2022-05-23T00:46:27.302572Z",
     "iopub.status.idle": "2022-05-23T00:46:29.116737Z",
     "shell.execute_reply": "2022-05-23T00:46:29.115758Z",
     "shell.execute_reply.started": "2022-05-23T00:46:27.302777Z"
    }
   },
   "outputs": [],
   "source": [
    "# Y tokeniser\n",
    "y_tokenizer = Tokenizer()\n",
    "y_tokenizer.fit_on_texts(list(y_train))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:29.118226Z",
     "iopub.status.busy": "2022-05-23T00:46:29.117977Z",
     "iopub.status.idle": "2022-05-23T00:46:31.728585Z",
     "shell.execute_reply": "2022-05-23T00:46:31.727366Z",
     "shell.execute_reply.started": "2022-05-23T00:46:29.118186Z"
    }
   },
   "outputs": [],
   "source": [
    "# one-hot-encoding\n",
    "y_train_sequence = y_tokenizer.texts_to_sequences(y_train)\n",
    "validation_y_sequence = y_tokenizer.texts_to_sequences(validation_y)\n",
    "\n",
    "# padding \n",
    "y_train_padded = pad_sequences(y_train_sequence, maxlen=max_summary_len, padding='post')\n",
    "validation_y_padded = pad_sequences(validation_y_sequence, maxlen=max_summary_len, padding='post')\n",
    "\n",
    "y_size = len(y_tokenizer.word_index) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:31.730152Z",
     "iopub.status.busy": "2022-05-23T00:46:31.729883Z",
     "iopub.status.idle": "2022-05-23T00:46:35.670614Z",
     "shell.execute_reply": "2022-05-23T00:46:35.669617Z",
     "shell.execute_reply.started": "2022-05-23T00:46:31.730121Z"
    }
   },
   "outputs": [],
   "source": [
    "# removing summary which only has sostok & eostok\n",
    "def remove_indices(sequence):\n",
    "    lst = []\n",
    "    for idx in range(len(sequence)):\n",
    "        cnt=0\n",
    "        for val in sequence[idx]:\n",
    "            if val!=0:\n",
    "                cnt+=1\n",
    "        if cnt==2:\n",
    "            lst.append(i)\n",
    "    return lst\n",
    "\n",
    "\n",
    "remove_train_indexes = remove_indices(y_train_padded)\n",
    "remove_val_indexes = remove_indices(validation_y_padded)\n",
    "\n",
    "y_train_padded = np.delete(y_train_padded, remove_train_indexes, axis=0)\n",
    "x_train_padded = np.delete(x_train_padded, remove_train_indexes, axis=0)\n",
    "\n",
    "validation_y_padded = np.delete(validation_y_padded, remove_val_indexes, axis=0)\n",
    "validation_x_padded = np.delete(validation_x_padded, remove_val_indexes, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:35.672043Z",
     "iopub.status.busy": "2022-05-23T00:46:35.671790Z",
     "iopub.status.idle": "2022-05-23T00:46:35.676130Z",
     "shell.execute_reply": "2022-05-23T00:46:35.675510Z",
     "shell.execute_reply.started": "2022-05-23T00:46:35.672015Z"
    }
   },
   "outputs": [],
   "source": [
    "dim = 240\n",
    "embedding_dim = 300\n",
    "num_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:46:35.677950Z",
     "iopub.status.busy": "2022-05-23T00:46:35.677338Z",
     "iopub.status.idle": "2022-05-23T00:47:54.911332Z",
     "shell.execute_reply": "2022-05-23T00:47:54.910466Z",
     "shell.execute_reply.started": "2022-05-23T00:46:35.677904Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_embedding_matrix(word_tokeniser, dimesions, size=None):\n",
    "    glove_file = '../input/glove6b/glove.6B.300d.txt'\n",
    "\n",
    "    word_embeddings_idx= {}\n",
    "\n",
    "    present = 0\n",
    "    absent = 0\n",
    "    \n",
    "    wrd_indices = word_tokeniser.word_index\n",
    "    vocabulary_words = wrd_indices.keys()\n",
    "    tokens = len(vocabulary_words) + 2 if not size else size\n",
    "\n",
    "    word_embedding_matrix = np.zeros((tokens, dimesions))\n",
    "\n",
    "    with open(glove_file) as file:\n",
    "        for line in file:\n",
    "\n",
    "            # Get word value and its coeeficients\n",
    "            word_value, coefficient = line.split(maxsplit=1)\n",
    "\n",
    "            # Convert String to Numpy Array\n",
    "            coefficient = np.fromstring(coefficient, \"f\", sep=\" \")\n",
    "\n",
    "            # Creating embedding dictionary\n",
    "            word_embeddings_idx[word_value] = coefficient\n",
    "\n",
    "    for word, idx in wrd_indices.items():\n",
    "\n",
    "        vector = word_embeddings_idx.get(word)\n",
    "\n",
    "        if vector is not None:\n",
    "\n",
    "            word_embedding_matrix[idx] = vector\n",
    "\n",
    "            present = present+1\n",
    "        else:\n",
    "            absent = absent+1\n",
    "\n",
    "    return word_embedding_matrix\n",
    "\n",
    "\n",
    "x_embedding_matrix = get_embedding_matrix(x_tokenizer, embedding_dim, x_size)\n",
    "y_embedding_matrix = get_embedding_matrix(y_tokenizer, embedding_dim, y_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:47:54.912924Z",
     "iopub.status.busy": "2022-05-23T00:47:54.912577Z",
     "iopub.status.idle": "2022-05-23T00:47:54.918277Z",
     "shell.execute_reply": "2022-05-23T00:47:54.917171Z",
     "shell.execute_reply.started": "2022-05-23T00:47:54.912886Z"
    }
   },
   "outputs": [],
   "source": [
    "print(x_embedding_matrix.shape)\n",
    "print(y_embedding_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:47:54.919841Z",
     "iopub.status.busy": "2022-05-23T00:47:54.919603Z",
     "iopub.status.idle": "2022-05-23T00:47:54.935610Z",
     "shell.execute_reply": "2022-05-23T00:47:54.934481Z",
     "shell.execute_reply.started": "2022-05-23T00:47:54.919796Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_lstm_model(embedding_dim, dim, max_text_len, xsize, ysize,x, y):\n",
    "    with tpu_strategy.scope():\n",
    "\n",
    "        # Encoder\n",
    "        # Defining input of Encoder\n",
    "        enc_input = Input(shape=(max_text_len, ))\n",
    "\n",
    "        # Encoder Embeddings\n",
    "        input_embeddings = Embedding(input_dim =xsize, output_dim=embedding_dim,\n",
    "                                     embeddings_initializer=tf.keras.initializers.Constant(x),\n",
    "                                   trainable=False)(enc_input)\n",
    "\n",
    "        dec_embedding = Embedding(ysize, embedding_dim, embeddings_initializer=tf.keras.initializers.Constant(y),\n",
    "                              trainable=True)\n",
    "\n",
    "        # Encoder 1\n",
    "        lstm1 = LSTM(dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "        \n",
    "        out1 = lstm1(input_embeddings)[0]\n",
    "\n",
    "        lstm2 = LSTM(dim, return_sequences=True, return_state=True, dropout=0.4, recurrent_dropout=0.4)\n",
    "        end_out, *states = lstm2(out1)\n",
    "\n",
    "        # Decoder\n",
    "        dec_input = Input(shape=(None, ))\n",
    "\n",
    "        # Creating Embeddings for decoder\n",
    "        decoder_embedding = dec_embedding(dec_input)\n",
    "\n",
    "        # Decoder 1 - LSTM\n",
    "        dec_lstm = LSTM(dim,return_sequences=True, return_state=True, dropout=0.4,recurrent_dropout=0.4)\n",
    "\n",
    "        dec_output, *decoder_last_states = dec_lstm(decoder_embedding, initial_state=states)\n",
    "\n",
    "        # dense layer\n",
    "        decoder_dense = TimeDistributed(Dense(ysize, activation='softmax'))\n",
    "        dec_output = decoder_dense(dec_output)\n",
    "\n",
    "        model = Model([enc_input, dec_input], dec_output)\n",
    "        model.compile(\n",
    "            optimizer=tf.keras.optimizers.RMSprop(learning_rate=0.001),\n",
    "            loss='sparse_categorical_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'model': model,\n",
    "            'inputs': {'encoder': enc_input,'decoder': dec_input\n",
    "            },\n",
    "            'outputs': {'encoder': end_out,'decoder': dec_output\n",
    "            },\n",
    "            'states': {'encoder': states,'decoder': decoder_last_states\n",
    "            },\n",
    "            'layers': {'decoder': { 'embedding': dec_embedding,'last_dec_lstm': dec_lstm,'dense': decoder_dense\n",
    "                }\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:47:54.937565Z",
     "iopub.status.busy": "2022-05-23T00:47:54.936894Z",
     "iopub.status.idle": "2022-05-23T00:47:59.265311Z",
     "shell.execute_reply": "2022-05-23T00:47:59.264409Z",
     "shell.execute_reply.started": "2022-05-23T00:47:54.937534Z"
    }
   },
   "outputs": [],
   "source": [
    "seq2seq = build_lstm_model(embedding_dim, dim, max_text_len, x_size, y_size,x_embedding_matrix, y_embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:47:59.267025Z",
     "iopub.status.busy": "2022-05-23T00:47:59.266743Z",
     "iopub.status.idle": "2022-05-23T00:47:59.273036Z",
     "shell.execute_reply": "2022-05-23T00:47:59.272237Z",
     "shell.execute_reply.started": "2022-05-23T00:47:59.266988Z"
    }
   },
   "outputs": [],
   "source": [
    "seq2seq['layers']['decoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:47:59.274554Z",
     "iopub.status.busy": "2022-05-23T00:47:59.274334Z",
     "iopub.status.idle": "2022-05-23T00:47:59.286336Z",
     "shell.execute_reply": "2022-05-23T00:47:59.285309Z",
     "shell.execute_reply.started": "2022-05-23T00:47:59.274529Z"
    }
   },
   "outputs": [],
   "source": [
    "model = seq2seq['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:47:59.288742Z",
     "iopub.status.busy": "2022-05-23T00:47:59.288438Z",
     "iopub.status.idle": "2022-05-23T00:47:59.299071Z",
     "shell.execute_reply": "2022-05-23T00:47:59.298218Z",
     "shell.execute_reply.started": "2022-05-23T00:47:59.288703Z"
    }
   },
   "outputs": [],
   "source": [
    "enc_input = seq2seq['inputs']['encoder']\n",
    "encoder_output = seq2seq['outputs']['encoder']\n",
    "encoder_final_states = seq2seq['states']['encoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:47:59.300733Z",
     "iopub.status.busy": "2022-05-23T00:47:59.300430Z",
     "iopub.status.idle": "2022-05-23T00:47:59.310261Z",
     "shell.execute_reply": "2022-05-23T00:47:59.309613Z",
     "shell.execute_reply.started": "2022-05-23T00:47:59.300694Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_input = seq2seq['inputs']['decoder']\n",
    "decoder_output = seq2seq['outputs']['decoder']\n",
    "decoder_last_states = seq2seq['states']['decoder']\n",
    "decoder_embedding_layer = seq2seq['layers']['decoder']['embedding']\n",
    "last_decoder_lstm = seq2seq['layers']['decoder']['last_dec_lstm']\n",
    "decoder_dense = seq2seq['layers']['decoder']['dense']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:47:59.312847Z",
     "iopub.status.busy": "2022-05-23T00:47:59.311801Z",
     "iopub.status.idle": "2022-05-23T00:47:59.331386Z",
     "shell.execute_reply": "2022-05-23T00:47:59.330352Z",
     "shell.execute_reply.started": "2022-05-23T00:47:59.312806Z"
    }
   },
   "outputs": [],
   "source": [
    "model.layers[-2].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:47:59.333078Z",
     "iopub.status.busy": "2022-05-23T00:47:59.332707Z",
     "iopub.status.idle": "2022-05-23T00:47:59.341570Z",
     "shell.execute_reply": "2022-05-23T00:47:59.340503Z",
     "shell.execute_reply.started": "2022-05-23T00:47:59.333036Z"
    }
   },
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, min_lr=0.000001, verbose=1),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a `tuple` instead of `list` in `validation_parameter` in `model.fit()`, to know the reason reading this [post](https://stackoverflow.com/questions/61586981/valueerror-layer-sequential-20-expects-1-inputs-but-it-received-2-input-tensor)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:47:59.343841Z",
     "iopub.status.busy": "2022-05-23T00:47:59.343239Z",
     "iopub.status.idle": "2022-05-23T00:51:18.238026Z",
     "shell.execute_reply": "2022-05-23T00:51:18.236807Z",
     "shell.execute_reply.started": "2022-05-23T00:47:59.343799Z"
    }
   },
   "outputs": [],
   "source": [
    "final_model = model.fit(\n",
    "    [x_train_padded, y_train_padded[:, :-1]],\n",
    "    y_train_padded.reshape(y_train_padded.shape[0], y_train_padded.shape[1], 1)[:, 1:],\n",
    "    epochs=num_epochs,\n",
    "    batch_size=128 * tpu_strategy.num_replicas_in_sync,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=(\n",
    "        [validation_x_padded, validation_y_padded[:, :-1]],\n",
    "        validation_y_padded.reshape(validation_y_padded.shape[0], validation_y_padded.shape[1], 1)[:, 1:]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting model's performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:18.241060Z",
     "iopub.status.busy": "2022-05-23T00:51:18.240657Z",
     "iopub.status.idle": "2022-05-23T00:51:18.517168Z",
     "shell.execute_reply": "2022-05-23T00:51:18.516143Z",
     "shell.execute_reply.started": "2022-05-23T00:51:18.241014Z"
    }
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "plt.plot(final_model.history['accuracy'][1:], label='train acc')\n",
    "plt.plot(final_model.history['val_accuracy'], label='val')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Accuracy value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:18.518586Z",
     "iopub.status.busy": "2022-05-23T00:51:18.518319Z",
     "iopub.status.idle": "2022-05-23T00:51:18.755245Z",
     "shell.execute_reply": "2022-05-23T00:51:18.754328Z",
     "shell.execute_reply.started": "2022-05-23T00:51:18.518558Z"
    }
   },
   "outputs": [],
   "source": [
    "# Loss\n",
    "plt.plot(final_model.history['loss'][1:], label='train loss')\n",
    "plt.plot(final_model.history['val_loss'], label='val')\n",
    "plt.xlabel('Number of Epochs')\n",
    "plt.ylabel('Loss Value')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:18.757343Z",
     "iopub.status.busy": "2022-05-23T00:51:18.757007Z",
     "iopub.status.idle": "2022-05-23T00:51:18.762562Z",
     "shell.execute_reply": "2022-05-23T00:51:18.761666Z",
     "shell.execute_reply.started": "2022-05-23T00:51:18.757301Z"
    }
   },
   "outputs": [],
   "source": [
    "# Next, let’s build the dictionary to convert the index to word for target and source vocabulary:\n",
    "target_idx_word= y_tokenizer.index_word\n",
    "source_idx_word = x_tokenizer.index_word\n",
    "target_word_index = y_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:18.764626Z",
     "iopub.status.busy": "2022-05-23T00:51:18.764277Z",
     "iopub.status.idle": "2022-05-23T00:51:18.776820Z",
     "shell.execute_reply": "2022-05-23T00:51:18.775696Z",
     "shell.execute_reply.started": "2022-05-23T00:51:18.764587Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_seq2seq_model_with_just_lstm_inference(\n",
    "    max_text_len, latent_dim, encoder_input, encoder_output,\n",
    "    encoder_final_states, decoder_input, decoder_output,\n",
    "    decoder_embedding_layer, decoder_dense, last_decoder_lstm\n",
    "):\n",
    "    # Encode the input sequence to get the feature vector\n",
    "    encoder_model = Model(\n",
    "        inputs=encoder_input, outputs=[encoder_output] + encoder_final_states\n",
    "    )\n",
    "\n",
    "    # Decoder setup\n",
    "    # Below tensors will hold the states of the previous time step\n",
    "    decoder_state_input_h = Input(shape=(latent_dim, ))\n",
    "    decoder_state_input_c = Input(shape=(latent_dim, ))\n",
    "    decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim))\n",
    "\n",
    "    # Get the embeddings of the decoder sequence\n",
    "    decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "\n",
    "    # To predict the next word in the sequence, set the initial\n",
    "    # states to the states from the previous time step\n",
    "    decoder_output, *decoder_states = last_decoder_lstm(\n",
    "        decoder_embedding,\n",
    "        initial_state=[decoder_state_input_h, decoder_state_input_c]\n",
    "    )\n",
    "\n",
    "    # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "    decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "    # Final decoder model\n",
    "    decoder_model = Model(\n",
    "        [decoder_input] + [decoder_hidden_state_input, decoder_state_input_h, decoder_state_input_c], \n",
    "        [decoder_output] + decoder_states\n",
    "    )\n",
    "\n",
    "    return (encoder_model, decoder_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:18.778883Z",
     "iopub.status.busy": "2022-05-23T00:51:18.778449Z",
     "iopub.status.idle": "2022-05-23T00:51:18.792997Z",
     "shell.execute_reply": "2022-05-23T00:51:18.792306Z",
     "shell.execute_reply.started": "2022-05-23T00:51:18.778835Z"
    }
   },
   "outputs": [],
   "source": [
    "max_text_len = 42\n",
    "max_summary_len = 13\n",
    "latent_dim = 240\n",
    "embedding_dim = 300\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:18.794137Z",
     "iopub.status.busy": "2022-05-23T00:51:18.793920Z",
     "iopub.status.idle": "2022-05-23T00:51:18.959880Z",
     "shell.execute_reply": "2022-05-23T00:51:18.959037Z",
     "shell.execute_reply.started": "2022-05-23T00:51:18.794109Z"
    }
   },
   "outputs": [],
   "source": [
    "def build_hybrid_seq2seq_model_inference(\n",
    "    max_text_len, latent_dim, encoder_input, encoder_output,\n",
    "    encoder_final_states, decoder_input, decoder_output,\n",
    "    decoder_embedding_layer, decoder_dense, last_decoder_bi_lstm\n",
    "):\n",
    "\n",
    "    # Encode the input sequence to get the feature vector\n",
    "    encoder_model = Model(\n",
    "        inputs=encoder_input, outputs=[encoder_output] + encoder_final_states\n",
    "    )\n",
    "\n",
    "    # Decoder setup\n",
    "    # Below tensors will hold the states of the previous time step\n",
    "    decoder_state_forward_input_h = Input(shape=(latent_dim, ))\n",
    "    decoder_state_forward_input_c = Input(shape=(latent_dim, ))\n",
    "    # decoder_state_backward_input_h = Input(shape=(latent_dim, ))\n",
    "    # decoder_state_backward_input_c = Input(shape=(latent_dim, ))\n",
    "\n",
    "    # Create the hidden input layer with twice the latent dimension,\n",
    "    # since we are using bi - directional LSTM's we will get \n",
    "    # two hidden states and two cell states\n",
    "    decoder_hidden_state_input = Input(shape=(max_text_len, latent_dim * 2))\n",
    "\n",
    "    decoder_initial_state = [\n",
    "        decoder_state_forward_input_h, decoder_state_forward_input_c,\n",
    "        #decoder_state_backward_input_h, decoder_state_backward_input_c\n",
    "    ]\n",
    "\n",
    "    # Get the embeddings of the decoder sequence\n",
    "    decoder_embedding = decoder_embedding_layer(decoder_input)\n",
    "\n",
    "    # To predict the next word in the sequence, set the initial\n",
    "    # states to the states from the previous time step\n",
    "    decoder_output, *decoder_states = last_decoder_bi_lstm(\n",
    "        decoder_embedding, initial_state=decoder_initial_state\n",
    "    )\n",
    "\n",
    "    # A dense softmax layer to generate prob dist. over the target vocabulary\n",
    "    decoder_output = decoder_dense(decoder_output)\n",
    "\n",
    "    # Final decoder model\n",
    "    decoder_model = Model(\n",
    "        [decoder_input] + [decoder_hidden_state_input] + decoder_initial_state,\n",
    "        [decoder_output] + decoder_states\n",
    "    )\n",
    "\n",
    "    return (encoder_model, decoder_model)\n",
    "encoder_model, decoder_model = build_seq2seq_model_with_just_lstm_inference(\n",
    "    max_text_len, latent_dim, enc_input, encoder_output,\n",
    "    encoder_final_states, decoder_input, decoder_output,\n",
    "    decoder_embedding_layer, decoder_dense, last_decoder_lstm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:18.961467Z",
     "iopub.status.busy": "2022-05-23T00:51:18.961116Z",
     "iopub.status.idle": "2022-05-23T00:51:19.130559Z",
     "shell.execute_reply": "2022-05-23T00:51:19.127819Z",
     "shell.execute_reply.started": "2022-05-23T00:51:18.961434Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_model, decoder_model = build_seq2seq_model_with_just_lstm_inference(\n",
    "    max_text_len, dim, enc_input, encoder_output,\n",
    "    encoder_final_states, decoder_input, decoder_output,\n",
    "    decoder_embedding_layer, decoder_dense, last_decoder_lstm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:19.132413Z",
     "iopub.status.busy": "2022-05-23T00:51:19.132058Z",
     "iopub.status.idle": "2022-05-23T00:51:19.146861Z",
     "shell.execute_reply": "2022-05-23T00:51:19.143717Z",
     "shell.execute_reply.started": "2022-05-23T00:51:19.132368Z"
    }
   },
   "outputs": [],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:19.150254Z",
     "iopub.status.busy": "2022-05-23T00:51:19.149875Z",
     "iopub.status.idle": "2022-05-23T00:51:19.165430Z",
     "shell.execute_reply": "2022-05-23T00:51:19.164434Z",
     "shell.execute_reply.started": "2022-05-23T00:51:19.150179Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:19.166981Z",
     "iopub.status.busy": "2022-05-23T00:51:19.166665Z",
     "iopub.status.idle": "2022-05-23T00:51:19.178121Z",
     "shell.execute_reply": "2022-05-23T00:51:19.177089Z",
     "shell.execute_reply.started": "2022-05-23T00:51:19.166942Z"
    }
   },
   "outputs": [],
   "source": [
    "decoder_model.layers[-3].input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:19.180453Z",
     "iopub.status.busy": "2022-05-23T00:51:19.179859Z",
     "iopub.status.idle": "2022-05-23T00:51:19.192898Z",
     "shell.execute_reply": "2022-05-23T00:51:19.191872Z",
     "shell.execute_reply.started": "2022-05-23T00:51:19.180404Z"
    }
   },
   "outputs": [],
   "source": [
    "def decode_sequence_seq2seq_model_with_lstm_model(\n",
    "    input_sequence, encoder_model, decoder_model\n",
    "):\n",
    "    # Encode the input as state vectors.\n",
    "    e_out, e_h, e_c = encoder_model.predict(input_sequence)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1))\n",
    "\n",
    "    # Populate the first word of target sequence with the start word.\n",
    "    target_seq[0, 0] = target_word_index[start_token]\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + [e_out, e_h, e_c]\n",
    "        )\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_token = target_idx_word[sampled_token_index]\n",
    "\n",
    "        if sampled_token != end_token:\n",
    "            decoded_sentence += ' ' + sampled_token\n",
    "\n",
    "        # Exit condition: either hit max length or find stop word.\n",
    "        if (sampled_token == end_token) or (len(decoded_sentence.split()) >= (max_summary_len - 1)):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1))\n",
    "        target_seq[0, 0] = sampled_token_index\n",
    "\n",
    "        # Update internal states\n",
    "        e_h, e_c = h, c\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:19.197234Z",
     "iopub.status.busy": "2022-05-23T00:51:19.196841Z",
     "iopub.status.idle": "2022-05-23T00:51:19.206615Z",
     "shell.execute_reply": "2022-05-23T00:51:19.205655Z",
     "shell.execute_reply.started": "2022-05-23T00:51:19.197123Z"
    }
   },
   "outputs": [],
   "source": [
    "def seq2summary(input_sequence):\n",
    "    val = ''\n",
    "    for i in input_sequence:\n",
    "        if (\n",
    "            (i != 0 and i != target_word_index[start_token]) and\n",
    "            (i != target_word_index[end_token])\n",
    "        ):\n",
    "            val = val + target_idx_word[i] + ' '\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:19.208245Z",
     "iopub.status.busy": "2022-05-23T00:51:19.207948Z",
     "iopub.status.idle": "2022-05-23T00:51:19.221577Z",
     "shell.execute_reply": "2022-05-23T00:51:19.220308Z",
     "shell.execute_reply.started": "2022-05-23T00:51:19.208160Z"
    }
   },
   "outputs": [],
   "source": [
    "def seq2text(input_sequence):\n",
    "    val = ''\n",
    "    for i in input_sequence:\n",
    "        if i != 0:\n",
    "            val = val + source_idx_word[i] + ' '\n",
    "    return val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:19.223142Z",
     "iopub.status.busy": "2022-05-23T00:51:19.222884Z",
     "iopub.status.idle": "2022-05-23T00:51:19.240664Z",
     "shell.execute_reply": "2022-05-23T00:51:19.239538Z",
     "shell.execute_reply.started": "2022-05-23T00:51:19.223114Z"
    }
   },
   "outputs": [],
   "source": [
    "def predict_text(text, decode_sequence, encoder_model, decoder_model):\n",
    "    org_txt = text\n",
    "    \n",
    "    txt_lst = org_txt.split()\n",
    "\n",
    "    text = clean_text([text])\n",
    "\n",
    "    if len(txt_lst) <= max_text_len:\n",
    "        text = lemmatise_words(text)\n",
    "        text = clean_text(text)\n",
    "        text = f'_START_ {text} _END_'\n",
    "        text = f'{start_token} {text} {end_token}'\n",
    "\n",
    "        seq = x_tokenizer.texts_to_sequences([' '.join(txt_lst)])\n",
    "        padded = pad_sequences(seq, maxlen=max_text_len, padding='post')\n",
    "        output_summary = decode_sequence(\n",
    "            padded.reshape(1, max_text_len), encoder_model, decoder_model\n",
    "        )\n",
    "        return output_summary\n",
    "    else:\n",
    "        output_summary = ''\n",
    "\n",
    "        while len(txt_lst) % max_text_len == 0:\n",
    "            txt_lst.append('')\n",
    "\n",
    "        lst_i = max_text_len\n",
    "        for i in range(0, len(txt_lst), max_text_len):\n",
    "            lst = org_txt.split()[i:i + lst_i]\n",
    "            local_txt= ' '.join(lst)\n",
    "            local_txt= ' '.join(\n",
    "                local_txt.split()\n",
    "            )  \n",
    "            local_txt= expand_contractions(local_txt)\n",
    "            local_txt= clean_text(local_txt)  \n",
    "            local_txt= f'_START_ {local_txt} _END_'\n",
    "            local_txt= f'{start_token} {local_txt} {end_token}'\n",
    "            # Convert to Sequence\n",
    "            _seq = x_tokenizer.texts_to_sequences([local_txt])\n",
    "            # Convert to Padded\n",
    "            _padded = pad_sequences(_seq, maxlen=max_text_len, padding='post')\n",
    "            # predictions\n",
    "            _pred = decode_sequence(\n",
    "                _padded.reshape(1, max_text_len), encoder_model, decoder_model\n",
    "            )\n",
    "            \n",
    "            # Output summary\n",
    "            output_summary += ' ' + ' '.join(_pred.split()[1:-2])\n",
    "            output_summary = ' '.join(output_summary.split())\n",
    "\n",
    "        return output_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:19.242318Z",
     "iopub.status.busy": "2022-05-23T00:51:19.242060Z",
     "iopub.status.idle": "2022-05-23T00:51:32.900054Z",
     "shell.execute_reply": "2022-05-23T00:51:32.898289Z",
     "shell.execute_reply.started": "2022-05-23T00:51:19.242289Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing on training data\n",
    "for i in range(0, 15):\n",
    "    print(f\"# {i+1} News text: \", seq2text(x_train_padded[i]))\n",
    "    print(\"Original summary text: \", seq2summary(y_train_padded[i]))\n",
    "    print(\n",
    "        \"Predicted summary text \",\n",
    "        decode_sequence_seq2seq_model_with_lstm_model(\n",
    "            x_train_padded[i].reshape(1, max_text_len), encoder_model,\n",
    "            decoder_model\n",
    "        )\n",
    "    )\n",
    "    print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:32.901615Z",
     "iopub.status.busy": "2022-05-23T00:51:32.901392Z",
     "iopub.status.idle": "2022-05-23T00:51:57.272343Z",
     "shell.execute_reply": "2022-05-23T00:51:57.271567Z",
     "shell.execute_reply.started": "2022-05-23T00:51:32.901590Z"
    }
   },
   "outputs": [],
   "source": [
    "# Testing on validation data\n",
    "original = []\n",
    "predicted = []\n",
    "for i in range(0, 15):\n",
    "    print(f\"# {i+1} News Text: \", seq2text(validation_x_padded[i]))\n",
    "    print(\"Original summary Text: \", seq2summary(validation_y_padded[i]))\n",
    "    print(\n",
    "        \"Predicted summary Text: \",\n",
    "        decode_sequence_seq2seq_model_with_lstm_model(\n",
    "            validation_x_padded[i].reshape(1, max_text_len), encoder_model,\n",
    "            decoder_model\n",
    "        )\n",
    "    )\n",
    "    original.append(seq2summary(validation_y_padded[i]))\n",
    "    predicted.append(decode_sequence_seq2seq_model_with_lstm_model(\n",
    "            validation_x_padded[i].reshape(1, max_text_len), encoder_model,\n",
    "            decoder_model\n",
    "        ))\n",
    "    print(\"*\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-23T00:51:57.273658Z",
     "iopub.status.busy": "2022-05-23T00:51:57.273444Z"
    }
   },
   "outputs": [],
   "source": [
    "# HDF5 format Saving the model\n",
    "model.save('model.h5')    \n",
    "encoder_model.save('encoder_model.h5')\n",
    "decoder_model.save('decoder_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install rouge_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "metric = load_metric(\"rouge\")\n",
    "\n",
    "def calc_rouge_scores(candidates, references):\n",
    "    result = metric.compute(predictions=candidates, references=references, use_stemmer=True)\n",
    "    result = {key: round(value.mid.fmeasure * 100, 1) for key, value in result.items()}\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range (15):\n",
    "    print(f\"First {i+1} senctences: Scores {calc_rouge_scores(original, predicted)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
